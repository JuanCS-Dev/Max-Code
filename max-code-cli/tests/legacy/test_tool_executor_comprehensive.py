"""
Comprehensive Tool Executor Tests - CRITICAL for Agent Execution

Biblical Foundation:
"Provai todas as coisas; retende o que √© bom." (1 Tessalonicenses 5:21)
Test all things - validate completeness.

OBJETIVO:
Test core/deter_agent/execution/tool_executor.py comprehensively
Focus on DAILY USE scenarios and reliability.

TEST CATEGORIES:
1. Tool Registration (5 tests)
2. Tool Execution (8 tests)
3. Error Handling (7 tests)
4. Result Validation (4 tests)
5. Concurrent Execution (3 tests)
6. Resource Management (4 tests)
7. Real Tools Integration (7 tests)
8. Constitutional Validation (4 tests)

Total: 42 comprehensive tests

"O que as tuas m√£os encontrarem para fazer, faze-o com toda a tua for√ßa"
(Eclesiastes 9:10)
"""

import sys
import os
import pytest
import tempfile
import threading
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add parent to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.deter_agent.execution.tool_executor import (
    ToolExecutor,
    Tool,
    ToolType,
    ToolStatus,
    ToolResult,
    create_bash_tool,
    create_file_read_tool,
)
from core.tools import (
    FileReader, FileWriter, FileEditor,
    GlobTool, GrepTool
)


# ============================================================================
# CATEGORY 1: TOOL REGISTRATION (5 tests)
# ============================================================================

class TestToolRegistration:
    """Test tool registration functionality"""

    def test_register_single_tool(self):
        """Test registering a single tool"""
        executor = ToolExecutor()

        tool = Tool(
            name="test_tool",
            type=ToolType.BASH,
            description="Test tool",
            parameters={'command': 'echo test'}
        )

        executor.register_tool(tool)

        assert "test_tool" in executor.registered_tools
        assert executor.registered_tools["test_tool"].name == "test_tool"
        assert executor.registered_tools["test_tool"].type == ToolType.BASH
        print("‚úì Single tool registration successful")

    def test_register_multiple_tools(self):
        """Test registering multiple tools"""
        executor = ToolExecutor()

        tools = [
            Tool("tool1", ToolType.BASH, "Tool 1", {'command': 'echo 1'}),
            Tool("tool2", ToolType.FILE_READ, "Tool 2", {'file_path': '/tmp/test'}),
            Tool("tool3", ToolType.GLOB, "Tool 3", {'pattern': '*.py'}),
        ]

        for tool in tools:
            executor.register_tool(tool)

        assert len(executor.registered_tools) == 3
        assert "tool1" in executor.registered_tools
        assert "tool2" in executor.registered_tools
        assert "tool3" in executor.registered_tools
        print("‚úì Multiple tool registration successful")

    def test_register_tool_overwrites_existing(self):
        """Test that re-registering a tool overwrites the previous one"""
        executor = ToolExecutor()

        tool1 = Tool("test", ToolType.BASH, "First", {'command': 'echo 1'})
        tool2 = Tool("test", ToolType.BASH, "Second", {'command': 'echo 2'})

        executor.register_tool(tool1)
        executor.register_tool(tool2)

        assert executor.registered_tools["test"].description == "Second"
        assert executor.registered_tools["test"].parameters['command'] == 'echo 2'
        print("‚úì Tool overwrite successful")

    def test_register_tools_all_types(self):
        """Test registering tools of all types"""
        executor = ToolExecutor()

        tool_types = [
            (ToolType.BASH, {'command': 'echo test'}),
            (ToolType.FILE_READ, {'file_path': '/tmp/test'}),
            (ToolType.FILE_WRITE, {'file_path': '/tmp/test', 'content': 'test'}),
            (ToolType.FILE_EDIT, {'file_path': '/tmp/test', 'old_string': 'a', 'new_string': 'b'}),
            (ToolType.GLOB, {'pattern': '*.py'}),
            (ToolType.GREP, {'pattern': 'test'}),
            (ToolType.API_CALL, {'url': 'https://api.example.com'}),
            (ToolType.SEARCH, {'pattern': 'test'}),
        ]

        for i, (tool_type, params) in enumerate(tool_types):
            tool = Tool(f"tool_{i}", tool_type, f"Tool {i}", params)
            executor.register_tool(tool)

        assert len(executor.registered_tools) == len(tool_types)
        print(f"‚úì All {len(tool_types)} tool types registered successfully")

    def test_helper_functions_create_tools(self):
        """Test helper functions for creating tools"""
        bash_tool = create_bash_tool("test_bash", "echo hello")
        read_tool = create_file_read_tool("test_read", "/tmp/test.txt")

        assert bash_tool.name == "test_bash"
        assert bash_tool.type == ToolType.BASH
        assert read_tool.name == "test_read"
        assert read_tool.type == ToolType.FILE_READ
        print("‚úì Helper functions work correctly")


# ============================================================================
# CATEGORY 2: TOOL EXECUTION (8 tests)
# ============================================================================

class TestToolExecution:
    """Test tool execution functionality"""

    def test_execute_bash_simple_command(self):
        """Test executing a simple bash command"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("echo_test", "echo 'Hello World'")
        executor.register_tool(tool)

        result = executor.execute("echo_test")

        assert result.status == ToolStatus.SUCCESS
        assert "Hello World" in result.output
        assert result.execution_time > 0
        print(f"‚úì Bash execution successful ({result.execution_time:.3f}s)")

    def test_execute_bash_with_parameters_override(self):
        """Test executing bash with parameter override"""
        executor = ToolExecutor(safe_mode=False)

        tool = Tool(
            "dynamic_echo",
            ToolType.BASH,
            "Echo with dynamic command",
            {'command': 'echo default'}
        )
        executor.register_tool(tool)

        result = executor.execute("dynamic_echo", {'command': 'echo override'})

        assert result.status == ToolStatus.SUCCESS
        assert "override" in result.output
        assert "default" not in result.output
        print("‚úì Parameter override successful")

    def test_execute_nonexistent_tool(self):
        """Test executing a tool that doesn't exist"""
        executor = ToolExecutor()

        result = executor.execute("nonexistent_tool")

        assert result.status == ToolStatus.FAILURE
        assert "not registered" in result.error
        assert executor.stats['failed_executions'] == 1
        print("‚úì Nonexistent tool handled correctly")

    def test_execute_with_validation_enabled(self):
        """Test execution with validation enabled"""
        executor = ToolExecutor(safe_mode=True)

        tool = Tool(
            "safe_echo",
            ToolType.BASH,
            "Safe echo",
            {'command': 'echo safe'},
            requires_validation=True
        )
        executor.register_tool(tool)

        result = executor.execute("safe_echo")

        assert result.status == ToolStatus.SUCCESS
        print("‚úì Validation passed for safe command")

    def test_execute_with_timeout(self):
        """Test execution respects timeout"""
        executor = ToolExecutor(safe_mode=False)

        tool = Tool(
            "sleep_tool",
            ToolType.BASH,
            "Sleep command",
            {'command': 'sleep 5', 'timeout': 0.5}
        )
        executor.register_tool(tool)

        result = executor.execute("sleep_tool")

        assert result.status == ToolStatus.TIMEOUT
        assert "timed out" in result.error.lower()
        assert result.execution_time < 2.0  # Should fail quickly
        print(f"‚úì Timeout enforced correctly ({result.execution_time:.3f}s)")

    def test_execute_tracks_statistics(self):
        """Test that execution updates statistics"""
        executor = ToolExecutor(safe_mode=False)

        success_tool = create_bash_tool("success", "echo success")
        fail_tool = create_bash_tool("fail", "exit 1")

        executor.register_tool(success_tool)
        executor.register_tool(fail_tool)

        executor.execute("success")
        executor.execute("fail")

        stats = executor.get_stats()

        assert stats['total_executions'] == 2
        assert stats['successful_executions'] == 1
        assert stats['failed_executions'] == 1
        assert stats['success_rate'] == 50.0
        print("‚úì Statistics tracking working correctly")

    def test_execute_creates_audit_trail(self):
        """Test that execution creates audit trail (P4)"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("audit_test", "echo audit")
        executor.register_tool(tool)

        executor.execute("audit_test")

        history = executor.get_execution_history()

        assert len(history) == 1
        assert history[0].tool_name == "audit_test"
        assert history[0].timestamp is not None
        assert isinstance(history[0].to_dict(), dict)
        print("‚úì Audit trail created (P4 - Rastreabilidade)")

    def test_execute_multiple_times_sequential(self):
        """Test executing same tool multiple times sequentially"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("repeat", "echo repeat")
        executor.register_tool(tool)

        results = []
        for _ in range(5):
            result = executor.execute("repeat")
            results.append(result)

        assert all(r.status == ToolStatus.SUCCESS for r in results)
        assert executor.stats['total_executions'] == 5
        assert executor.stats['successful_executions'] == 5
        print("‚úì Sequential execution successful (5 runs)")


# ============================================================================
# CATEGORY 3: ERROR HANDLING (7 tests)
# ============================================================================

class TestErrorHandling:
    """Test error handling and recovery"""

    def test_handle_bash_command_failure(self):
        """Test handling bash command that returns non-zero exit code"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("fail_cmd", "exit 42")
        executor.register_tool(tool)

        result = executor.execute("fail_cmd")

        assert result.status == ToolStatus.FAILURE
        assert "code 42" in result.error
        assert result.output is None
        print("‚úì Bash failure handled correctly")

    def test_handle_file_not_found_error(self):
        """Test handling file not found error"""
        executor = ToolExecutor()

        tool = Tool(
            "read_missing",
            ToolType.FILE_READ,
            "Read nonexistent file",
            {'file_path': '/nonexistent/file.txt'}
        )
        executor.register_tool(tool)

        result = executor.execute("read_missing")

        # With self-correction disabled for this test
        executor.enable_self_correction = False
        result = executor.execute("read_missing")

        assert result.status == ToolStatus.FAILURE
        assert result.error is not None
        print("‚úì File not found handled correctly")

    def test_handle_permission_denied_error(self):
        """Test handling permission denied error"""
        executor = ToolExecutor()

        # Try to write to system path
        tool = Tool(
            "write_etc",
            ToolType.FILE_WRITE,
            "Write to /etc",
            {'file_path': '/etc/test.txt', 'content': 'test'}
        )
        executor.register_tool(tool)

        result = executor.execute("write_etc")

        assert result.status == ToolStatus.BLOCKED
        assert "system path" in result.error.lower()
        assert executor.stats['blocked_executions'] == 1
        print("‚úì Permission denied handled correctly (P5 - Systemic Impact)")

    def test_handle_timeout_gracefully(self):
        """Test that timeout is handled gracefully without crashing"""
        executor = ToolExecutor(safe_mode=False)

        tool = Tool(
            "timeout_test",
            ToolType.BASH,
            "Long running command",
            {'command': 'sleep 10', 'timeout': 0.2}
        )
        executor.register_tool(tool)

        result = executor.execute("timeout_test")

        assert result.status == ToolStatus.TIMEOUT
        assert result.error is not None
        assert result.execution_time < 1.0
        print("‚úì Timeout handled gracefully")

    def test_handle_invalid_parameters(self):
        """Test handling invalid parameters"""
        executor = ToolExecutor()

        tool = Tool(
            "missing_param",
            ToolType.FILE_READ,
            "Read without path",
            {'limit': 10}  # Missing required 'file_path'
        )
        executor.register_tool(tool)

        result = executor.execute("missing_param")

        assert result.status == ToolStatus.FAILURE
        print("‚úì Invalid parameters handled correctly")

    def test_handle_dangerous_command_blocked(self):
        """Test that dangerous commands are blocked"""
        executor = ToolExecutor(safe_mode=True)

        dangerous_commands = [
            "rm -rf /",
            "format c:",
            "mkfs /dev/sda",
            "dd if=/dev/zero of=/dev/sda"
        ]

        for cmd in dangerous_commands:
            tool = Tool(
                f"dangerous_{hash(cmd)}",
                ToolType.BASH,
                "Dangerous command",
                {'command': cmd}
            )
            executor.register_tool(tool)
            result = executor.execute(tool.name)

            assert result.status == ToolStatus.BLOCKED
            assert "dangerous" in result.error.lower()

        print(f"‚úì All {len(dangerous_commands)} dangerous commands blocked (P5)")

    def test_error_recovery_with_retry(self):
        """Test error recovery with retry mechanism"""
        executor = ToolExecutor(safe_mode=False)

        # Create a tool that might fail transiently
        tool = create_bash_tool("flaky", "python3 -c 'import random; exit(random.randint(0,1))'")
        executor.register_tool(tool)

        # Execute multiple times, at least one should succeed
        results = [executor.execute("flaky") for _ in range(10)]

        # Should have mix of success and failure
        success_count = sum(1 for r in results if r.status == ToolStatus.SUCCESS)

        assert success_count >= 0  # At least some may succeed
        assert len(results) == 10
        print(f"‚úì Retry mechanism tested ({success_count}/10 succeeded)")


# ============================================================================
# CATEGORY 4: RESULT VALIDATION (4 tests)
# ============================================================================

class TestResultValidation:
    """Test result validation and formatting"""

    def test_result_contains_all_fields(self):
        """Test that ToolResult contains all required fields"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("test", "echo test")
        executor.register_tool(tool)

        result = executor.execute("test")

        assert hasattr(result, 'tool_name')
        assert hasattr(result, 'tool_type')
        assert hasattr(result, 'status')
        assert hasattr(result, 'output')
        assert hasattr(result, 'error')
        assert hasattr(result, 'execution_time')
        assert hasattr(result, 'metadata')
        assert hasattr(result, 'timestamp')
        print("‚úì ToolResult has all required fields")

    def test_result_to_dict_conversion(self):
        """Test ToolResult can be converted to dict"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("test", "echo test")
        executor.register_tool(tool)

        result = executor.execute("test")
        result_dict = result.to_dict()

        assert isinstance(result_dict, dict)
        assert 'tool_name' in result_dict
        assert 'status' in result_dict
        assert 'execution_time' in result_dict
        assert 'timestamp' in result_dict
        print("‚úì to_dict() conversion successful")

    def test_result_output_truncation(self):
        """Test that large output is truncated in to_dict()"""
        executor = ToolExecutor(safe_mode=False)

        # Create command with large output
        tool = create_bash_tool("large_output", "python3 -c 'print(\"x\" * 1000)'")
        executor.register_tool(tool)

        result = executor.execute("large_output")
        result_dict = result.to_dict()

        # to_dict() truncates output to 200 chars
        assert len(result_dict['output']) <= 200
        print("‚úì Output truncation working correctly")

    def test_execution_history_limited(self):
        """Test that execution history can be limited"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("test", "echo test")
        executor.register_tool(tool)

        # Execute 20 times
        for _ in range(20):
            executor.execute("test")

        # Get last 5
        history = executor.get_execution_history(limit=5)

        assert len(history) == 5
        assert all(isinstance(r, ToolResult) for r in history)
        print("‚úì Execution history limit working correctly")


# ============================================================================
# CATEGORY 5: CONCURRENT EXECUTION (3 tests)
# ============================================================================

class TestConcurrentExecution:
    """Test concurrent tool execution"""

    def test_concurrent_execution_thread_safety(self):
        """Test that executor is thread-safe for concurrent execution"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("concurrent", "echo concurrent")
        executor.register_tool(tool)

        def run_tool():
            return executor.execute("concurrent")

        # Run 10 executions concurrently
        with ThreadPoolExecutor(max_workers=5) as pool:
            futures = [pool.submit(run_tool) for _ in range(10)]
            results = [f.result() for f in as_completed(futures)]

        assert len(results) == 10
        assert all(r.status == ToolStatus.SUCCESS for r in results)
        assert executor.stats['total_executions'] == 10
        print("‚úì Concurrent execution is thread-safe (10 concurrent runs)")

    def test_concurrent_different_tools(self):
        """Test concurrent execution of different tools"""
        executor = ToolExecutor(safe_mode=False)

        for i in range(5):
            tool = create_bash_tool(f"tool_{i}", f"echo tool_{i}")
            executor.register_tool(tool)

        def run_tool(tool_name):
            return executor.execute(tool_name)

        # Run different tools concurrently
        with ThreadPoolExecutor(max_workers=5) as pool:
            futures = [pool.submit(run_tool, f"tool_{i}") for i in range(5)]
            results = [f.result() for f in as_completed(futures)]

        assert len(results) == 5
        assert all(r.status == ToolStatus.SUCCESS for r in results)
        print("‚úì Concurrent execution of different tools successful")

    def test_concurrent_statistics_accuracy(self):
        """Test that statistics remain accurate under concurrent load"""
        executor = ToolExecutor(safe_mode=False)

        success_tool = create_bash_tool("success", "echo ok")
        fail_tool = create_bash_tool("fail", "exit 1")

        executor.register_tool(success_tool)
        executor.register_tool(fail_tool)

        def run_tools():
            executor.execute("success")
            executor.execute("fail")

        # Run concurrently
        with ThreadPoolExecutor(max_workers=3) as pool:
            futures = [pool.submit(run_tools) for _ in range(5)]
            [f.result() for f in as_completed(futures)]

        stats = executor.get_stats()

        # 5 runs √ó 2 tools = 10 total
        assert stats['total_executions'] == 10
        assert stats['successful_executions'] == 5
        assert stats['failed_executions'] == 5
        print("‚úì Statistics accurate under concurrent load")


# ============================================================================
# CATEGORY 6: RESOURCE MANAGEMENT (4 tests)
# ============================================================================

class TestResourceManagement:
    """Test resource management and cleanup"""

    def test_executor_initialization(self):
        """Test executor initialization with different modes"""
        executor_safe = ToolExecutor(safe_mode=True, enable_self_correction=True)
        executor_unsafe = ToolExecutor(safe_mode=False, enable_self_correction=False)

        assert executor_safe.safe_mode is True
        assert executor_safe.enable_self_correction is True
        assert executor_unsafe.safe_mode is False
        assert executor_unsafe.enable_self_correction is False

        # Check file tools initialized
        assert executor_safe.file_reader is not None
        assert executor_safe.file_writer is not None
        assert executor_safe.glob_tool is not None
        print("‚úì Executor initialization successful")

    def test_statistics_tracking(self):
        """Test statistics tracking and calculation"""
        executor = ToolExecutor(safe_mode=False)

        tool1 = create_bash_tool("t1", "echo 1")
        tool2 = create_bash_tool("t2", "exit 1")

        executor.register_tool(tool1)
        executor.register_tool(tool2)

        executor.execute("t1")
        executor.execute("t2")
        executor.execute("t1")

        stats = executor.get_stats()

        assert stats['total_executions'] == 3
        assert stats['successful_executions'] == 2
        assert stats['failed_executions'] == 1
        assert 'success_rate' in stats
        assert 'avg_execution_time' in stats
        assert stats['success_rate'] == pytest.approx(66.66, rel=0.1)
        print("‚úì Statistics calculation correct")

    def test_execution_history_persistence(self):
        """Test that execution history persists across multiple executions"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("test", "echo test")
        executor.register_tool(tool)

        # Execute multiple times
        for i in range(5):
            executor.execute("test")

        history = executor.get_execution_history(limit=100)

        assert len(history) == 5
        assert len(executor.execution_history) == 5
        print("‚úì Execution history persists correctly")

    def test_memory_cleanup_for_large_outputs(self):
        """Test that large outputs don't cause memory issues"""
        executor = ToolExecutor(safe_mode=False)

        # Create tool with large output
        tool = create_bash_tool(
            "large",
            "python3 -c 'print(\"x\" * 100000)'"
        )
        executor.register_tool(tool)

        # Execute multiple times
        for _ in range(10):
            result = executor.execute("large")
            assert result.status == ToolStatus.SUCCESS

        # History should contain all executions
        assert len(executor.execution_history) == 10
        print("‚úì Large outputs handled without memory issues")


# ============================================================================
# CATEGORY 7: REAL TOOLS INTEGRATION (7 tests)
# ============================================================================

class TestRealToolsIntegration:
    """Test integration with real file tools"""

    def setup_method(self):
        """Setup test files"""
        self.temp_dir = tempfile.mkdtemp()
        self.test_file = os.path.join(self.temp_dir, "test.txt")

        with open(self.test_file, 'w') as f:
            f.write("Line 1\nLine 2\nLine 3\nTest content\n")

    def teardown_method(self):
        """Cleanup test files"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_real_file_reader(self):
        """Test FileReader integration"""
        executor = ToolExecutor()

        tool = Tool(
            "read_file",
            ToolType.FILE_READ,
            "Read test file",
            {'file_path': self.test_file}
        )
        executor.register_tool(tool)

        result = executor.execute("read_file")

        assert result.status == ToolStatus.SUCCESS
        assert "Line 1" in result.output
        assert "Test content" in result.output
        print("‚úì FileReader integration successful")

    def test_real_file_writer(self):
        """Test FileWriter integration"""
        executor = ToolExecutor()

        output_file = os.path.join(self.temp_dir, "output.txt")

        tool = Tool(
            "write_file",
            ToolType.FILE_WRITE,
            "Write test file",
            {'file_path': output_file, 'content': 'Hello World'}
        )
        executor.register_tool(tool)

        result = executor.execute("write_file")

        assert result.status == ToolStatus.SUCCESS
        assert os.path.exists(output_file)

        with open(output_file) as f:
            content = f.read()
        assert content == "Hello World"
        print("‚úì FileWriter integration successful")

    def test_real_file_editor(self):
        """Test FileEditor integration"""
        executor = ToolExecutor()

        tool = Tool(
            "edit_file",
            ToolType.FILE_EDIT,
            "Edit test file",
            {
                'file_path': self.test_file,
                'old_string': 'Line 2',
                'new_string': 'Modified Line 2'
            }
        )
        executor.register_tool(tool)

        result = executor.execute("edit_file")

        assert result.status == ToolStatus.SUCCESS
        assert "1 occurrence" in result.output

        with open(self.test_file) as f:
            content = f.read()
        assert "Modified Line 2" in content
        print("‚úì FileEditor integration successful")

    def test_real_glob_tool(self):
        """Test GlobTool integration"""
        executor = ToolExecutor()

        # Create some test files
        for i in range(3):
            path = os.path.join(self.temp_dir, f"test_{i}.txt")
            with open(path, 'w') as f:
                f.write(f"Test {i}")

        tool = Tool(
            "glob_files",
            ToolType.GLOB,
            "Find txt files",
            {'pattern': '*.txt', 'path': self.temp_dir}
        )
        executor.register_tool(tool)

        result = executor.execute("glob_files")

        assert result.status == ToolStatus.SUCCESS
        assert "Found 4 files" in result.output  # 3 + original test.txt
        print("‚úì GlobTool integration successful")

    def test_real_grep_tool(self):
        """Test GrepTool integration"""
        executor = ToolExecutor()

        tool = Tool(
            "grep_content",
            ToolType.GREP,
            "Search in files",
            {
                'pattern': 'Test content',
                'path': self.temp_dir,
                'output_mode': 'content'
            }
        )
        executor.register_tool(tool)

        result = executor.execute("grep_content")

        assert result.status == ToolStatus.SUCCESS
        assert "Test content" in result.output
        print("‚úì GrepTool integration successful")

    def test_real_file_read_with_limits(self):
        """Test FileReader with offset and limit"""
        executor = ToolExecutor()

        tool = Tool(
            "read_limited",
            ToolType.FILE_READ,
            "Read with limits",
            {'file_path': self.test_file, 'offset': 1, 'limit': 2}
        )
        executor.register_tool(tool)

        result = executor.execute("read_limited")

        assert result.status == ToolStatus.SUCCESS
        # Should only read 2 lines starting from line 1
        lines = result.output.strip().split('\n')
        assert len(lines) <= 2
        print("‚úì FileReader with limits successful")

    def test_real_tools_error_handling(self):
        """Test real tools handle errors correctly"""
        executor = ToolExecutor()

        # Try to read non-existent file
        tool = Tool(
            "read_missing",
            ToolType.FILE_READ,
            "Read missing file",
            {'file_path': '/nonexistent/file.txt'}
        )
        executor.register_tool(tool)

        # Disable self-correction for this test
        executor.enable_self_correction = False
        result = executor.execute("read_missing")

        assert result.status == ToolStatus.FAILURE
        assert result.error is not None
        print("‚úì Real tools error handling successful")


# ============================================================================
# CATEGORY 8: CONSTITUTIONAL VALIDATION (4 tests)
# ============================================================================

class TestConstitutionalValidation:
    """Test Constitutional AI validation (P1-P6)"""

    def test_p2_api_validation_check(self):
        """Test P2: API validation check"""
        executor = ToolExecutor(safe_mode=True)

        tool = Tool(
            "api_call",
            ToolType.API_CALL,
            "Call external API",
            {'url': 'https://api.example.com'},
            requires_validation=True
        )
        executor.register_tool(tool)

        result = executor.execute("api_call")

        # API calls should be validated (currently placeholder)
        assert result is not None
        print("‚úì P2 API validation check in place")

    def test_p4_rastreabilidade_audit_trail(self):
        """Test P4: Rastreabilidade - audit trail"""
        executor = ToolExecutor(safe_mode=False)

        tool = create_bash_tool("audit_test", "echo audit")
        executor.register_tool(tool)

        result = executor.execute("audit_test")

        # Check audit trail
        assert len(executor.execution_history) > 0
        assert executor.execution_history[-1].tool_name == "audit_test"
        assert executor.execution_history[-1].timestamp is not None

        # Check to_dict for logging
        audit_dict = result.to_dict()
        assert 'timestamp' in audit_dict
        assert 'tool_name' in audit_dict
        print("‚úì P4 Rastreabilidade audit trail working")

    def test_p5_systemic_impact_validation(self):
        """Test P5: Systemic impact - dangerous operations blocked"""
        executor = ToolExecutor(safe_mode=True)

        dangerous_operations = [
            ('bash', {'command': 'rm -rf /'}),
            ('file_write', {'file_path': '/etc/passwd', 'content': 'test'}),
            ('file_write', {'file_path': '/bin/test', 'content': 'test'}),
            ('file_write', {'file_path': '/usr/local/test', 'content': 'test'}),
        ]

        for i, (op_type, params) in enumerate(dangerous_operations):
            tool_type = ToolType.BASH if op_type == 'bash' else ToolType.FILE_WRITE
            tool = Tool(f"dangerous_{i}", tool_type, "Dangerous op", params)
            executor.register_tool(tool)

            result = executor.execute(f"dangerous_{i}")

            assert result.status == ToolStatus.BLOCKED
            assert executor.stats['blocked_executions'] > 0

        print(f"‚úì P5 Systemic impact validation blocked {len(dangerous_operations)} dangerous ops")

    def test_p5_self_correction_integration(self):
        """Test P5: Autocorre√ß√£o Humilde - self-correction integration"""
        executor = ToolExecutor(safe_mode=False, enable_self_correction=True)

        # Check self-correction engine initialized
        assert executor.self_correction_engine is not None
        assert executor.enable_self_correction is True

        # Stats should track self-corrections
        assert 'self_corrections' in executor.stats
        assert executor.stats['self_corrections'] == 0

        print("‚úì P5 Self-correction integration in place")


# ============================================================================
# INTEGRATION TEST: END-TO-END WORKFLOW
# ============================================================================

class TestEndToEndWorkflow:
    """Test complete end-to-end workflow"""

    def test_complete_workflow(self):
        """Test complete workflow: register, execute, validate, audit"""
        print("\n" + "="*70)
        print("END-TO-END WORKFLOW TEST")
        print("="*70)

        executor = ToolExecutor(safe_mode=True, enable_self_correction=False)

        # 1. REGISTER multiple tools
        tools = [
            create_bash_tool("echo_hello", "echo Hello"),
            create_bash_tool("echo_world", "echo World"),
            create_bash_tool("pwd", "pwd"),
        ]

        for tool in tools:
            executor.register_tool(tool)

        print(f"\n‚úì Registered {len(tools)} tools")

        # 2. EXECUTE tools
        results = []
        for tool in tools:
            result = executor.execute(tool.name)
            results.append(result)

        print(f"‚úì Executed {len(results)} tools")

        # 3. VALIDATE results
        success_count = sum(1 for r in results if r.status == ToolStatus.SUCCESS)
        assert success_count == len(tools)

        print(f"‚úì All {success_count} executions successful")

        # 4. CHECK audit trail (P4)
        history = executor.get_execution_history()
        assert len(history) == len(tools)

        print(f"‚úì Audit trail contains {len(history)} entries")

        # 5. PRINT statistics
        executor.print_stats()

        stats = executor.get_stats()
        assert stats['success_rate'] == 100.0

        print("\n‚úì END-TO-END WORKFLOW COMPLETE")
        print("="*70)


# ============================================================================
# TEST RUNNER
# ============================================================================

def run_all_tests():
    """Run all test categories"""
    print("\n" + "="*70)
    print("COMPREHENSIVE TOOL EXECUTOR TESTS")
    print("="*70)
    print("\nBiblical Foundation:")
    print('"Provai todas as coisas; retende o que √© bom." (1 Tessalonicenses 5:21)')
    print("="*70)

    test_categories = [
        ("Tool Registration", TestToolRegistration),
        ("Tool Execution", TestToolExecution),
        ("Error Handling", TestErrorHandling),
        ("Result Validation", TestResultValidation),
        ("Concurrent Execution", TestConcurrentExecution),
        ("Resource Management", TestResourceManagement),
        ("Real Tools Integration", TestRealToolsIntegration),
        ("Constitutional Validation", TestConstitutionalValidation),
        ("End-to-End Workflow", TestEndToEndWorkflow),
    ]

    total_tests = 0
    passed_tests = 0
    failed_tests = 0

    for category_name, test_class in test_categories:
        print(f"\n{'='*70}")
        print(f"CATEGORY: {category_name}")
        print('='*70)

        # Get all test methods
        test_methods = [m for m in dir(test_class) if m.startswith('test_')]

        for method_name in test_methods:
            total_tests += 1
            print(f"\n[{total_tests}] Running {method_name}...")

            try:
                # Create instance and run test
                test_instance = test_class()

                # Run setup if exists
                if hasattr(test_instance, 'setup_method'):
                    test_instance.setup_method()

                # Run test
                method = getattr(test_instance, method_name)
                method()

                # Run teardown if exists
                if hasattr(test_instance, 'teardown_method'):
                    test_instance.teardown_method()

                passed_tests += 1
                print(f"    ‚úì PASSED")

            except Exception as e:
                failed_tests += 1
                print(f"    ‚úó FAILED: {str(e)}")

    # Print summary
    print("\n" + "="*70)
    print("TEST SUMMARY")
    print("="*70)
    print(f"Total tests:  {total_tests}")
    print(f"Passed:       {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
    print(f"Failed:       {failed_tests}")
    print("="*70)

    if failed_tests == 0:
        print("\n‚úÖ ALL TESTS PASSED!")
        print("üèéÔ∏è PAGANI: Tool Executor is PRODUCTION READY!")
    else:
        print(f"\n‚ö†Ô∏è  {failed_tests} tests failed")

    return passed_tests == total_tests


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
